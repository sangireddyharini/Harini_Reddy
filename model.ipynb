{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu95ZPvUTDwDRhwy4hE+EG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangireddyharini/Harini_Reddy/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-DS7p4GI1P0",
        "outputId": "f7317e08-9a06-41c2-9b8b-6c4c5caed9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar value : 5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "scalar = np.array(5)\n",
        "print(\"Scalar value :\", scalar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = np.array([1, 2, 3])\n",
        "print(\"Vector values:\",vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZiJ8T_7Jjh_",
        "outputId": "2380c51c-5d10-4d65-b2db-fca34da562fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector values: [1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "scalar = np.array(5)\n",
        "print(\"Scalar value:\", scalar)\n",
        "vector = np.array([1, 2, 3])\n",
        "print(\"Vector values:\", vector)\n",
        "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"2D Tensor (Matrix):\\n\",matrix)\n",
        "tensor_3d = np.array([[[1, 2, 3,], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
        "print(\"3D Tensor:\\n\", tensor_3d)\n",
        "tensor_4d = np.random.rand(2, 3, 4, 5)\n",
        "print(\"Shape of 4D Tensor:\", tensor_4d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wczmlLbtKC6G",
        "outputId": "f5db6a66-e831-4838-9070-42865e525ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar value: 5\n",
            "Vector values: [1 2 3]\n",
            "2D Tensor (Matrix):\n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "3D Tensor:\n",
            " [[[ 1  2  3]\n",
            "  [ 4  5  6]]\n",
            "\n",
            " [[ 7  8  9]\n",
            "  [10 11 12]]]\n",
            "Shape of 4D Tensor: (2, 3, 4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "def generate_data(num_samples):\n",
        "  x1 = np.random.randint(0, 100, size=num_samples)\n",
        "  x2 = np.random.randint(0, 100, size=num_samples)\n",
        "  y = x1 + x2\n",
        "  return np.vstack((x1, x2)).T, y\n",
        "num_samples = 10000\n",
        "x, y = generate_data(num_samples)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=2, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=72, batch_size=32, validation_split=0.2)\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "sample_input = np.array([[23, 45]])\n",
        "predicted_sum = model.predict(sample_input)\n",
        "print(f'Predicted Sum: {predicted_sum[0][0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHdEFrTzMziu",
        "outputId": "06d3de0f-8c3c-4d34-d4a5-f1edc6eeeeac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/72\n",
            "200/200 [==============================] - 2s 4ms/step - loss: 1947.7345 - val_loss: 69.1046\n",
            "Epoch 2/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 31.7276 - val_loss: 11.6164\n",
            "Epoch 3/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 4.5782 - val_loss: 1.3600\n",
            "Epoch 4/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.8700 - val_loss: 0.5485\n",
            "Epoch 5/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.3439 - val_loss: 0.1833\n",
            "Epoch 6/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.1434 - val_loss: 0.1060\n",
            "Epoch 7/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.0821\n",
            "Epoch 8/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.0659\n",
            "Epoch 9/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0552\n",
            "Epoch 10/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0539 - val_loss: 0.0486\n",
            "Epoch 11/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0444\n",
            "Epoch 12/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0436 - val_loss: 0.0417\n",
            "Epoch 13/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0407 - val_loss: 0.0397\n",
            "Epoch 14/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0384 - val_loss: 0.0375\n",
            "Epoch 15/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0363 - val_loss: 0.0357\n",
            "Epoch 16/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0336\n",
            "Epoch 17/72\n",
            "200/200 [==============================] - 1s 2ms/step - loss: 0.0324 - val_loss: 0.0315\n",
            "Epoch 18/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0303 - val_loss: 0.0295\n",
            "Epoch 19/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0282 - val_loss: 0.0274\n",
            "Epoch 20/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0254\n",
            "Epoch 21/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0230\n",
            "Epoch 22/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0210\n",
            "Epoch 23/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0199 - val_loss: 0.0187\n",
            "Epoch 24/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0167\n",
            "Epoch 25/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.0160 - val_loss: 0.0149\n",
            "Epoch 26/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.0142 - val_loss: 0.0133\n",
            "Epoch 27/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.0125 - val_loss: 0.0114\n",
            "Epoch 28/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.0111 - val_loss: 0.0100\n",
            "Epoch 29/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0087\n",
            "Epoch 30/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0077\n",
            "Epoch 31/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0068\n",
            "Epoch 32/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0057\n",
            "Epoch 33/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 34/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0049 - val_loss: 0.0042\n",
            "Epoch 35/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0042 - val_loss: 0.0037\n",
            "Epoch 36/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 37/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 38/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 39/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 40/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0018\n",
            "Epoch 41/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 42/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 43/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 9.1810e-04\n",
            "Epoch 44/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 9.5625e-04 - val_loss: 7.1999e-04\n",
            "Epoch 45/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 7.5231e-04 - val_loss: 6.5135e-04\n",
            "Epoch 46/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 6.0826e-04 - val_loss: 4.5140e-04\n",
            "Epoch 47/72\n",
            "200/200 [==============================] - 1s 2ms/step - loss: 4.9449e-04 - val_loss: 4.4477e-04\n",
            "Epoch 48/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 3.7391e-04 - val_loss: 2.7318e-04\n",
            "Epoch 49/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 2.7802e-04 - val_loss: 2.0106e-04\n",
            "Epoch 50/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 2.1605e-04 - val_loss: 1.6752e-04\n",
            "Epoch 51/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 1.6252e-04 - val_loss: 1.1825e-04\n",
            "Epoch 52/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 1.2358e-04 - val_loss: 8.4094e-05\n",
            "Epoch 53/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 8.7957e-05 - val_loss: 6.3282e-05\n",
            "Epoch 54/72\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 7.5780e-05 - val_loss: 4.7234e-05\n",
            "Epoch 55/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 5.4494e-05 - val_loss: 3.3244e-05\n",
            "Epoch 56/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 7.8601e-05 - val_loss: 3.9199e-05\n",
            "Epoch 57/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 3.6101e-05 - val_loss: 2.0795e-05\n",
            "Epoch 58/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 6.7258e-05 - val_loss: 1.7210e-05\n",
            "Epoch 59/72\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 2.7925e-05 - val_loss: 1.7267e-05\n",
            "Epoch 60/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 1.0785e-04 - val_loss: 9.9035e-06\n",
            "Epoch 61/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 2.4482e-05 - val_loss: 2.6283e-05\n",
            "Epoch 62/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 2.5523e-04 - val_loss: 1.0669e-05\n",
            "Epoch 63/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 7.1668e-05 - val_loss: 8.3077e-06\n",
            "Epoch 64/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0163\n",
            "Epoch 65/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 1.8744e-06\n",
            "Epoch 66/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 3.5274e-06 - val_loss: 1.4773e-06\n",
            "Epoch 67/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 1.0455e-05 - val_loss: 2.9564e-06\n",
            "Epoch 68/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 2.3827e-06 - val_loss: 1.3745e-05\n",
            "Epoch 69/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 5.1388e-06 - val_loss: 3.8474e-07\n",
            "Epoch 70/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 71/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 6.9269e-07\n",
            "Epoch 72/72\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 2.9936e-06 - val_loss: 3.2373e-06\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 5.0133e-06\n",
            "Test Loss: 5.013336249248823e-06\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "Predicted Sum: 67.99884796142578\n"
          ]
        }
      ]
    }
  ]
}